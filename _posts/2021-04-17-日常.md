---
layout:  post
title:  日常
subtitle:  pytorch自动微分机制
date:  2021-04-17
author:  LY
catalog:  true
tags:
    -深度学习框架
---

# pytorch的相关流程梳理

## 动态计算图的概念：

* pytorch的计算图由节点和边组成，节点表示张量和函数，边表示张量和函数之间的依赖关系。

* 计算图的正向传播是立即执行的，无需像静态图一样等计算图创造完毕才能输入数据得到输出。
* 计算图在反向传播后会立即销毁，比如使用backward进行反向传播或者torch.autograd.grad计算了梯度，那么创建的计算图会被立即销毁，释放内存，下次调用需要重新创建，如果需要保留计算图需要将retain_graph置为True。

## 自动微分机制：

* 一种通过backward方法在标量的张量上调用，求出对应自变量的梯度并保存在相应自变量的grad属性下，如果调用的张量不是标量，则需要传入一个和该张量相同形状的gradient参数张量，类似于caffe中的loss _weights，将后面的梯度按照相应的权重回传给前面的自变量。

  ```python
x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求
y = 2*torch.pow(x,2)
gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])
y.backward(gradient=gradient)
  ```

  还有另一种更直观的方法，用标量的反向传播实现非标量的反向传播：

  ```python
x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导y = 2*torch.pow(x,2)
gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])
z = torch.sum(y*gradient)
z.backward()
  ```

​		只有标量才可以进行直接调用backward方法，否则会先按照相应权重矩阵将非标量转换成标量再进行反向传播

* 另一种就是通过autograd.grad方法自动求导：

  ```python
  x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导
  y = 2*torch.pow(x,2)
gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])
dx_grad = torch.autograd.grad(y,x,create_graph=True,grad_outputs=gradient)#设置create_graph为True将允许创建更高阶的导数
dx2_grad = torch.autograd.grad(dx_grad,x)#求二阶导数
  ```
  
  

## 反向传播的细节

* backward函数调用的流程：
  1. 调用backward方法的张量将自己的梯度grad属性置为1
  2. 根据自身梯度以及关联的backward方法，计算出其对应自变量的梯度，并赋给相应张量的grad属性
  3. 当有多个因变量，则会将因变量计算出来的梯度进行累加赋给自变量的grad属性（就是因为累加的特性，所以张量的grad属性不会自动清零，需要手动清除或者调用统一清零的函数）

* 叶子节点和非叶子节点

  张量为叶子节点的条件：

  1. 是由用户直接创建的张量，而不是由某个Function计算出来的
  2. 该张量的requires_grad属性是True

  只有叶子节点的grad最终才会保留下来，这样的设计规则主要是节约内存和显存，如果需要保留所有中间计算结果的grad，则需要调用中间张量的retain_grad()方法

  ```python
  x = torch.tensor(3.0,requires_grad=True)
  y1 = x + 1
  y2 = 2*x
  y2.retain_grad()
  loss = (y1-y2)**2
  loss.retain_grad()
  loss.backward()
  print(loss.grad)
  ```

  