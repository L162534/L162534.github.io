---
layout:  post
title:  日常
subtitle:  Attention机制
date:  2021-04-27
author:  LY
catalog:  true
tags:
---

## 优点

* 一步到位的全局联系捕捉
* 并行计算减少模型训练时间
* 模型复杂度小，参数少

## 缺点

* 不能利用元素的顺序

## 键值对注意力模式

假设我们有一个键值对集合（key-value）来描述输入信息，那么N个输入信息就可以表示为(K, V) = [(k1,v1), (k2,v2),...,(kn,vn)]，其中key用来描述注意力分布，value用来描述聚合信息

我们可以将注意力机制看成一个软寻址操作：把输入信息（K，V）看成是存储器中存储的内容，Key代表地址Value代表值，当我们需要查询地址为Query的值时，如果是硬寻址就要返回Key=Query时的Value，在软寻址中，我们计算每个Key和Query的相似度，将相似度转换成权值乘以对应的Value，得到我们的Attention值

整个步骤可分成三个部分：

1. 用Query和Key计算相似度
   $$
   s_{i}=F(Q,k_{i})
   $$

2. 用softmax对相似度归一化计算权重
   $$
   a_{i}=softmax(s_{i})=\frac{exp(s_{i})}{\sum_{j=1}^{N}exp(s_j)}
   $$

3. 根据权重对Value加权求和，计算出Attention值
   $$
   Attention((K,V),Q)=\sum_{i=1}^{N}a_{i}v_{i}
   $$
   

![](D:\Documents\GitHub\L162534.github.io\img\attention1.png)