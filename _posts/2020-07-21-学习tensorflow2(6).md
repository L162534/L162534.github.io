---
layout:  post
title:  学习tensorflow2(6)
subtitle:  抓紧学！来不及了
date:  2020-07-23
author:  L&X
catalog:  true
tags:
    -python
    -tensorflow
---

## 复制文件路径的问题

尽量不要在文件的对象地址直接复制粘贴，对象地址有时候会携带控制字符，有时候看起来是一样的，其实字符串是不相等的。

```
‪C:/Users/Mark/Desktop/example.csv #文件的对象地址
202a433a2f55736572732f4d61726b2f4465736b746f702f6578616d706c652e637376 #转换成16进制
‪C:/Users/Mark/Desktop/example.csv #相同的字符串
433a2f55736572732f4d61726b2f4465736b746f702f6578616d706c652e637376 #转换成16进制
```

可以看出多了202a的控制字符，是windows用来强调字符排列顺序的控制字符，由于是隐形的表面上是看不出来的，如果测量字符长度也会发现对象地址比正常地址长一个单位。

## 准备训练集数据

* 训练集是csv文件加文件夹的形式（csv文件包含图片地址和label）

  1. 将图片地址改成符合路径的地址，下例中原数据集地址只有图片序号，经过修改生成新的符合条件的csv文件
  
      ```python
      path = "D:/Documents/PycharmProject/dataset/cifar-10/trainLabels.csv"
      path1 = "D:/Documents/PycharmProject/dataset/cifar-10/trainLabels1.csv"
      with open(path, 'r+') as f:
          reader = csv.reader(f)
          with open(path1, 'w+', newline='') as g:
              writer = csv.writer(g)
              for l in reader:
                  l_row = []
                  l_row.append('D:/Documents/PycharmProject/dataset/cifar-10/train/'+l[0]+'.png')
                  l_row.append(l[1])
                  writer.writerow(l_row)
      ```
      
  2. 将csv文件的读出，生成两个列表，包含图片地址列表和label列表
  
      ```python
      def read_csv(file):
          with open(file, 'r') as f:
              lines = f.readlines()[1:]
          tokens = np.array([l.rstrip().split(',') for l in lines])
          images_path = tokens[:, 0]
          images_label = tokens[:, 1]
          return images_path, images_label
      
      path_train, labels_train = read_csv(file_train)
      ```
  
  3. 生成图片数据集格式
  
      tf.data.Dataset.from_tensor_slices（）函数功能就是去掉列表的最外面一层维度，[[img1, label1], [img2, label2]]变成[img1,label1],[img2,label2]
  
      ```python
      path_train_ds = tf.data.Dataset.from_tensor_slices(path_train)
      ```
  
      再运用数据集的map函数通过加载图片的函数将图片地址数据集映射成图片数据集
  
      ```python
      def preprocess_image(image):
          image = image/255
          return image
      def load_and_process_image(path):
          image = tf.io.read_file(path)
          image = tf.image.decode_image(image, channels=3)
          image = tf.cast(image, tf.float32)
          image.set_shape([32, 32, 3])
          return preprocess_image(image)
      image_train_ds = path_train_ds.map(load_and_process_image)
      
      ```
  
  4. 生成onehot数据集格式
  
      先构造将标签字符串转换成onehot格式的函数
  
      ```python
      def label_to_onehot(list):
          onehot_list = []
          label_list = {
              'frog':0,
              'truck':1,
              'deer':2,
              'automobile':3,
              'bird':4,
              'horse':5,
              'ship':6,
              'cat':7,
              'dog':8,
              'airplane':9
          }
          for l in list:
              if l in label_list:
                  onehot = tf.keras.utils.to_categorical(label_list[l], len(label_list))
                  onehot_list.append(onehot)
              else: raise ValueError('can not recognize the label')
          return onehot_list
      ```
  
      先对label列表进行转换，再将转换后的onehot列表生成label数据集
  
      ```python
      labels_train = common.label_to_onehot(labels_train)
      label_train_ds = tf.data.Dataset.from_tensor_slices(labels_train)
      ```
  
  5. 将图片数据集与label数据集合并形成最终的训练数据集，并对数据集进行相应的设置
  
      ```python
      train_ds = tf.data.Dataset.zip((image_train_ds, label_train_ds))
      train_ds.batch(30)
      ```
  
  6. 同理设置验证集val_ds

## 自定义训练过程

1.构建单步训练，以batch为单位

```python
def train_step(model, features, labels, learn_rate,optimizer=None
               loss_func = tf.keras.losses.categorical_crossentropy,# 定义损失函数
               train_metric_loss=None, train_metric_accuracy=None):# 定义评估函数
    optimizer = optimizer# 定义优化器
    loss_func = loss_func
    with tf.GradientTape() as tape:
        predictions = model(features)
        loss = loss_func(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)#求解梯度
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))# 误差反向传播
    train_metric_loss.update_state(loss)# 将这一个batch的loss累加到当前epoch的总体loss上
    train_metric_accuracy.update_state(labels, predictions)# 将这一个batch的accuracy累加到当前epoch的总体accuracy上
```

**对于评估指标要用类方法，不能用函数，评估是针对整个epoch，包含多个batch，所以要将各个batch的指标进行累加再综合评估**

2.构建单步验证，以batch为单位

```python
def validate_step(model, features, labels,loss_func = tf.keras.losses.categorical_crossentropy,
                  validate_metric_loss=None, validate_metric_accuracy=None):
    loss_func = loss_func
    predictions = model(features)
    batch_loss = loss_func(labels, predictions)
    validate_metric_loss.update_state(batch_loss)
    validate_metric_accuracy.update_state(labels, predictions)
```

3.构建循环训练函数

```python
def train_model(model, train_ds, validate_ds, epochs, optimizer=None):
    train_metric_loss = tf.keras.metrics.Mean(name='train_loss')#训练中用于评估的损失函数
    train_metric_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')#训练中用于评估的精确度函数
    validate_metric_loss = tf.keras.metrics.Mean(name='validate_loss')#验证中用于评估的损失函数
    validate_metric_accuracy = tf.keras.metrics.CategoricalAccuracy(name='validate_loss')#验证中用于评估的精确度函数
    for epoch in tf.range(1, epochs+1):
        for features, labels in train_ds:
            train_step(model, features, labels, optimizer=optimizer,
                       train_metric_loss=train_metric_loss,
                       train_metric_accuracy=train_metric_accuracy)

        for features, labels in validate_ds:
            validate_step(model, features, labels, validate_metric_loss=validate_metric_loss,
                          validate_metric_accuracy=validate_metric_accuracy)
        logs = 'Epoch{},Loss:{}, Accuracy:{}, Valid Loss:{}, Valid Accuracy:{}'
        if epoch % 1 == 0:
            tf.print(tf.strings.format(logs, (epoch, train_metric_loss.result(), train_metric_accuracy.result(),validate_metric_loss.result(), validate_metric_accuracy.result())))#打印每个epoch的评估指标
        train_metric_loss.reset_states()#复位每个评估函数的值，以便下个epoch继续运行
        train_metric_accuracy.reset_states()
        validate_metric_loss.reset_states()
        validate_metric_accuracy.reset_states()
```

**注意这里的optimizer的优化器是同一个优化器，如果将optimizer定义在函数内部，每调用一次训练函数就会新建一个新的optimizer，导致loss只有第一次下降，后面又是新的optimizer，导致训练失败。**

