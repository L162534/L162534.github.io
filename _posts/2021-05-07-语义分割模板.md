---
layout:  post
title:  语义分割模板
subtitle:  keep moving
date:  2021-05-07
author:  LY
catalog:  true
tags:
    -语义分割
---

# 目录结构

```
​```
pytorch-template/
│
├── train.py - main script to start training
├── inference.py - inference using a trained model
├── trainer.py - the main trained
├── config.json - holds configuration for training
│
├── base/ - abstract base classes
│   ├── base_data_loader.py
│   ├── base_model.py
│   ├── base_dataset.py - All the data augmentations are implemented here
│   └── base_trainer.py
│
├── dataloader/ - loading the data for different segmentation datasets
│
├── models/ - contains semantic segmentation models
│
├── saved/
│   ├── runs/ - trained models are saved here
│   └── log/ - default logdir for tensorboard and logging output
│  
└── utils/ - small utility functions
    ├── losses.py - losses used in training the model
    ├── metrics.py - evaluation metrics used
    └── lr_scheduler - learning rate schedulers 
​```
```

# base

## base_dataset.py

基于torch.utils.data中的Dataset类，这次类的设计比上次Unet的Dataset设计更复杂，功能也多一些，更鲁棒一些：

1. 将图像增强的代码放在Dataset的内部
2. 实现\_\_getitem\_\_()和\_\_len\_\_()这两个必要的方法外，还添加了\_\_repr\_\_()方法，在运行代码时可以更加直观地看到数据集的属性
3. 将读取数据函数_load_data()作为接口函数让继承DataSet这个类的类去实现，提高了代码的普适性

root是根目录，split是VOC数据集里的参数，mean，std是对图像归一化需要用到的均值和方差，base_size是图片最长边的统一初始尺寸，augment决定是否对训练集进行图像增强，val决定是否生成验证集，crop_size是将图片修剪后的尺寸（如果尺寸不够就直接填充黑色），scale是对图像进行缩放的比例因子，flip，rotate，blur决定图像增强是否使用反转，旋转，滤波，return_id决定返回数据的格式是(image,label)还是(image,label,image_id)

```python
class BaseDataSet(Dataset):
    def __init__(self, root, split, mean, std, base_size=None, augment=True, val=False,
                crop_size=321, scale=True, flip=True, rotate=False, blur=False, return_id=False):
        self.root = root
        self.split = split
        self.mean = mean
        self.std = std
        self.augment = augment
        self.crop_size = crop_size
        if self.augment:
            self.base_size = base_size
            self.scale = scale
            self.flip = flip
            self.rotate = rotate
            self.blur = blur
        self.val = val
        self.files = []
        self._set_files()
        self.to_tensor = transforms.ToTensor()
        self.normalize = transforms.Normalize(mean, std)
        self.return_id = return_id

        cv2.setNumThreads(0)

    def _set_files(self):
        raise NotImplementedError
    
    def _load_data(self, index):
        raise NotImplementedError

    def _val_augmentation(self, image, label):
        if self.crop_size:
            h, w = label.shape
            # Scale the smaller side to crop size
            
            if h < w:
                h, w = (self.crop_size, int(self.crop_size * w / h))
            else:
                h, w = (int(self.crop_size * h / w), self.crop_size)

            image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LINEAR)
            label = Image.fromarray(label).resize((w, h), resample=Image.NEAREST)
            label = np.asarray(label, dtype=np.int32)

            # Center Crop
            
            h, w = label.shape
            start_h = (h - self.crop_size )// 2
            start_w = (w - self.crop_size )// 2
            end_h = start_h + self.crop_size
            end_w = start_w + self.crop_size
            image = image[start_h:end_h, start_w:end_w]
            label = label[start_h:end_h, start_w:end_w]
        return image, label

    def _augmentation(self, image, label):
        h, w, _ = image.shape
        # Scaling, we set the bigger to base size, and the smaller 
        # one is rescaled to maintain the same ratio, if we don't have any obj in the image, re-do the processing
        
        if self.base_size:
            if self.scale:
                longside = random.randint(int(self.base_size*0.5), int(self.base_size*2.0))
            else:
                longside = self.base_size
            h, w = (longside, int(1.0 * longside * w / h + 0.5)) if h > w else (int(1.0 * longside * h / w + 0.5), longside)
            image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LINEAR)
            label = cv2.resize(label, (w, h), interpolation=cv2.INTER_NEAREST)
    
        h, w, _ = image.shape
        # Rotate the image with an angle between -10 and 10
        
        if self.rotate:
            angle = random.randint(-10, 10)
            center = (w / 2, h / 2)
            rot_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            image = cv2.warpAffine(image, rot_matrix, (w, h), flags=cv2.INTER_LINEAR)#, borderMode=cv2.BORDER_REFLECT)
            label = cv2.warpAffine(label, rot_matrix, (w, h), flags=cv2.INTER_NEAREST)#,  borderMode=cv2.BORDER_REFLECT)

        # Padding to return the correct crop size
        
        if self.crop_size:
            pad_h = max(self.crop_size - h, 0)
            pad_w = max(self.crop_size - w, 0)
            pad_kwargs = {
                "top": 0,
                "bottom": pad_h,
                "left": 0,
                "right": pad_w,
                "borderType": cv2.BORDER_CONSTANT,}
            if pad_h > 0 or pad_w > 0:
                image = cv2.copyMakeBorder(image, value=0, **pad_kwargs)
                label = cv2.copyMakeBorder(label, value=0, **pad_kwargs)
            
            # Cropping 
            
            h, w, _ = image.shape
            start_h = random.randint(0, h - self.crop_size)
            start_w = random.randint(0, w - self.crop_size)
            end_h = start_h + self.crop_size
            end_w = start_w + self.crop_size
            image = image[start_h:end_h, start_w:end_w]
            label = label[start_h:end_h, start_w:end_w]

        # Random H flip
        
        if self.flip:
            if random.random() > 0.5:
                image = np.fliplr(image).copy()
                label = np.fliplr(label).copy()

        # Gaussian Blud (sigma between 0 and 1.5)
        
        if self.blur:
            sigma = random.random()
            ksize = int(3.3 * sigma)
            ksize = ksize + 1 if ksize % 2 == 0 else ksize
            image = cv2.GaussianBlur(image, (ksize, ksize), sigmaX=sigma, sigmaY=sigma, borderType=cv2.BORDER_REFLECT_101)
        return image, label
        
    def __len__(self):
        return len(self.files)

    def __getitem__(self, index):
        image, label, image_id = self._load_data(index)
        if self.val:
            image, label = self._val_augmentation(image, label)
        elif self.augment:
            image, label = self._augmentation(image, label)

        label = torch.from_numpy(np.array(label, dtype=np.int32)).long()
        image = Image.fromarray(np.uint8(image))
        if self.return_id:
            return  self.normalize(self.to_tensor(image)), label, image_id
        return self.normalize(self.to_tensor(image)), label

    def __repr__(self):
        fmt_str = "Dataset: " + self.__class__.__name__ + "\n"
        fmt_str += "    # data: {}\n".format(self.__len__())
        fmt_str += "    Split: {}\n".format(self.split)
        fmt_str += "    Root: {}".format(self.root)
        return fmt_str

```

## base_dataloader

dataloader就是将准备好的dataset按照一定的策略去取得索引然后调用dataset中的getitem函数，可以说dataloader就是dataset和sampler的结合体，dataset负责提供数据，sampler负责按照一定的策略提供索引，当然这里Sampler也可以是Iterbable类型的。

最常见的调用方法：

```python
dataset=Dataset()
dataloader = DataLoader(dataset=dataset)
for data_batch in dataloader:
	...
```

具体流程就是多次调用dataloader的\_\_iter\_\_()方法，而这个方法又调用 _BaseDataLoaderIter类的\_\_next\_\_()方法，该方法就是不断调用dataset的\_\_getitem\_\_()方法，然后通过collate_fn将其打包成一个batch数据

```python
torch.utils.data.DataLoader(dataset, batch_size,shuffle,sampler, batch_sampler, num_workers, collate_fn,pin_meory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator,prefetch_factor, persistent_workers)
```

* dataset: Dataset类型的对象
* batch_size:默认为1
* shuffle:是否将每个epoch的数据打乱
* sampler:可以为Sampler或者Iterable，用来定义采样数据的策略
* collate_fn:将每个样本采样后进行相应处理最后形成一个batch，自动将numpy的array格式转换成Tensor格式
* drop_last:是否丢弃最后不足一个batch的数据

下面是自己继承DataLoader自定义一个dataloader

```python
class BaseDataLoader(DataLoader):
    def __init__(self, dataset, batch_size, shuffle, num_workers, val_split=0.0):
        self.shuffle = shuffle
        self.dataset = dataset
        self.shuffle = shuffle
        self.dataset = dataset
        self.nbr_examples = len(dataset)
        if val_split: 
            self.train_sampler, self.val_sampler = self._split_sampler(val_split)
        else: 
            self.train_sampler, self.val_sampler = None, None

        self.init_kwargs = {
            'dataset': self.dataset,
            'batch_size': batch_size,
            'shuffle': self.shuffle,
            'num_workers': num_workers,
            'pin_memory': True
        }
        super(BaseDataLoader, self).__init__(sampler=self.train_sampler, **self.init_kwargs)
        
    def _split_sampler(self, split):
        if split == 0.0:
            return None, None
        
        self.shuffle = False

        split_indx = int(self.nbr_examples * split)
        np.random.seed(0)
        
        indxs = np.arange(self.nbr_examples)
        np.random.shuffle(indxs)
        train_indxs = indxs[split_indx:]
        val_indxs = indxs[:split_indx]
        self.nbr_examples = len(train_indxs)

        train_sampler = SubsetRandomSampler(train_indxs)
        val_sampler = SubsetRandomSampler(val_indxs)
        return train_sampler, val_sampler
        def get_val_loader(self):
        if self.val_sampler is None:
            return None
        #self.init_kwargs['batch_size'] = 1
        
        return DataLoader(sampler=self.val_sampler, **self.init_kwargs)
```

为了提高gpu的利用率，在网络运算过程中，我们可以预先读取下一次迭代需要的数据，因此我们构造出一个DataPrefetcher，为了方便扩展，我们将之前用的dataloader作为其中的参数，修改代码时只需要修改dataloader相关调用的代码即可

```python
class DataPrefetcher(object):
    def __init__(self, loader, device, stop_after=None):
        self.loader = loader
        self.dataset = loader.dataset
        self.stream = torch.cuda.Stream()
        self.stop_after = stop_after
        self.next_input = None
        self.next_target = None
        self.device = device

    def __len__(self):
        return len(self.loader)

    def preload(self):
        try:
            self.next_input, self.next_target = next(self.loaditer)
        except StopIteration:
            self.next_input = None
            self.next_target = None
            return
        with torch.cuda.stream(self.stream):
            self.next_input = self.next_input.cuda(device=self.device, non_blocking=True)
            self.next_target = self.next_target.cuda(device=self.device, non_blocking=True)

    def __iter__(self):
        count = 0
        self.loaditer = iter(self.loader)
        self.preload()
        while self.next_input is not None:
            torch.cuda.current_stream().wait_stream(self.stream)
            input = self.next_input
            target = self.next_target
            self.preload()
            count += 1
            yield input, target
            if type(self.stop_after) is int and (count > self.stop_after):
                break

```

之前的代码可能是这种形式：

```python
for batch in dataloader:
	...
```

现在只需改成：

```python
batch = dataprefetcher.next()
while(batch != None):
	...
	batch = dataprefetcher.next()
```

 ## base_model

再nn.Module基础模块上加了logger属性，用来记录日志，添加了summary方法用来记录模型的训练参数大小，还有改写了\_\_str\_\_()方法，打印模型时会在原来的基础上额外显示可训练的参数大小

```python
import logging
import torch.nn as nn
import numpy as np

class BaseModel(nn.Module):
    def __init__(self):
        super(BaseModel, self).__init__()
        self.logger = logging.getLogger(self.__class__.__name__)

    def forward(self):
        raise NotImplementedError

    def summary(self):
        model_parameters = filter(lambda p: p.requires_grad, self.parameters())
        nbr_params = sum([np.prod(p.size()) for p in model_parameters])
        self.logger.info(f'Nbr of trainable parameters: {nbr_params}')

    def __str__(self):
        model_parameters = filter(lambda p: p.requires_grad, self.parameters())
        nbr_params = sum([np.prod(p.size()) for p in model_parameters])
        return super(BaseModel, self).__str__() + f'\nNbr of trainable parameters: {nbr_params}'
```

## base_trainer

这个类是用来配置整个训练过程的模板类，适用于所有任务类型的训练过程，需要自己实现的就是每个epoch的训练方法\_train\_epoch()、验证方法\_valid\_epoch()和评估方法\_eval\_epoch()

集成的方法包括：

1. cpu或者gpu的选择，同时还支持batchnorm的gpu并行
2. 训练器trainer的超参数的选择（解析json配置文件）：
   * 迭代次数
   * 保存目录
   * 每迭代多少次就保存一次模型
   * 评估指标是什么，同时当评估指标在一定迭代次数之内不变化不大时就停止训练
   * 是否启用tensorboard以及其对应的log_dir和log_per_iter
   * 是否启用validation以及其对应的val_per_epochs
3. dataloader的参数解析
4. 优化器的超参数设置
5. 损失函数的设置
6. 断点续训

```python
class BaseTrainer:
    def __init__(self, model, loss, resume, config, train_loader, val_loader=None, train_logger=None):
        self.model = model
        self.loss = loss
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.train_logger = train_logger
        self.logger = logging.getLogger(self.__class__.__name__)
        self.do_validation = self.config['trainer']['val']
        self.start_epoch = 1
        self.improved = False
        # SETTING THE DEVICE
        
        self.device, availble_gpus = self._get_available_devices(self.config['n_gpu'])
        if config["use_synch_bn"]:
            self.model = convert_model(self.model)
            self.model = DataParallelWithCallback(self.model, device_ids=availble_gpus)
        else:
            self.model = torch.nn.DataParallel(self.model, device_ids=availble_gpus)
        self.model.to(self.device)
        # CONFIGS
        
        cfg_trainer = self.config['trainer']
        self.epochs = cfg_trainer['epochs']
        self.save_period = cfg_trainer['save_period']
        # OPTIMIZER
        
        if self.config['optimizer']['differential_lr']:
            if isinstance(self.model, torch.nn.DataParallel):
                trainable_params = [{'params': filter(lambda p:p.requires_grad, self.model.module.get_decoder_params())},
                                    {'params': filter(lambda p:p.requires_grad, self.model.module.get_backbone_params()), 
                                    'lr': config['optimizer']['args']['lr'] / 10}]
            else: 
                trainable_params = [{'params': filter(lambda p:p.requires_grad, self.model.get_decoder_params())},
                                    {'params': filter(lambda p:p.requires_grad, self.model.get_backbone_params()), 
                                    'lr': config['optimizer']['args']['lr'] / 10}]
        else:
            trainable_params = filter(lambda p:p.requires_grad, self.model.parameters())
        self.optimizer = get_instance(torch.optim, 'optimizer', config, trainable_params)
        self.lr_scheduler = getattr(utils.lr_scheduler, config['lr_scheduler']['type'])(self.optimizer, self.epochs, len(train_loader))
        # MONITORING
        
        self.monitor = cfg_trainer.get('monitor', 'off')
        if self.monitor == 'off':
            self.mnt_mode = 'off'
            self.mnt_best = 0
        else:
            self.mnt_mode, self.mnt_metric = self.monitor.split()
            assert self.mnt_mode in ['min', 'max']
            self.mnt_best = -math.inf if self.mnt_mode == 'max' else math.inf
            self.early_stoping = cfg_trainer.get('early_stop', math.inf)
        # CHECKPOINTS & TENSOBOARD
        
        start_time = datetime.datetime.now().strftime('%m-%d_%H-%M')
        self.checkpoint_dir = os.path.join(cfg_trainer['save_dir'], self.config['name'], start_time)
        helpers.dir_exists(self.checkpoint_dir)
        config_save_path = os.path.join(self.checkpoint_dir, 'config.json')
        with open(config_save_path, 'w') as handle:
            json.dump(self.config, handle, indent=4, sort_keys=True)

        writer_dir = os.path.join(cfg_trainer['log_dir'], self.config['name'], start_time)
        self.writer = tensorboard.SummaryWriter(writer_dir)

        if resume: self._resume_checkpoint(resume)

    def _get_available_devices(self, n_gpu):
        sys_gpu = torch.cuda.device_count()
        if sys_gpu == 0:
            self.logger.warning('No GPUs detected, using the CPU')
            n_gpu = 0
        elif n_gpu > sys_gpu:
            self.logger.warning(f'Nbr of GPU requested is {n_gpu} but only {sys_gpu} are available')
            n_gpu = sys_gpu
            
        device = torch.device('cuda:0' if n_gpu > 0 else 'cpu')
        self.logger.info(f'Detected GPUs: {sys_gpu} Requested: {n_gpu}')
        available_gpus = list(range(n_gpu))
        return device, available_gpus
    
    def train(self):
        for epoch in range(self.start_epoch, self.epochs+1):
            # RUN TRAIN (AND VAL)
            
            results = self._train_epoch(epoch)
            if self.do_validation and epoch % self.config['trainer']['val_per_epochs'] == 0:
                results = self._valid_epoch(epoch)
                # LOGGING INFO
                
                self.logger.info(f'\n         ## Info for epoch {epoch} ## ')
                for k, v in results.items():
                    self.logger.info(f'         {str(k):15s}: {v}')
            
            if self.train_logger is not None:
                log = {'epoch' : epoch, **results}
                self.train_logger.add_entry(log)
            # CHECKING IF THIS IS THE BEST MODEL (ONLY FOR VAL)
            
            if self.mnt_mode != 'off' and epoch % self.config['trainer']['val_per_epochs'] == 0:
                try:
                    if self.mnt_mode == 'min': self.improved = (log[self.mnt_metric] < self.mnt_best)
                    else: self.improved = (log[self.mnt_metric] > self.mnt_best)
                except KeyError:
                    self.logger.warning(f'The metrics being tracked ({self.mnt_metric}) has not been calculated. Training stops.')
                    break
                    
                if self.improved:
                    self.mnt_best = log[self.mnt_metric]
                    self.not_improved_count = 0
                else:
                    self.not_improved_count += 1

                if self.not_improved_count > self.early_stoping:
                    self.logger.info(f'\nPerformance didn\'t improve for {self.early_stoping} epochs')
                    self.logger.warning('Training Stoped')
                    break
            # SAVE CHECKPOINT
            
            if epoch % self.save_period == 0:
                self._save_checkpoint(epoch, save_best=self.improved)

    def _save_checkpoint(self, epoch, save_best=False):
        state = {
            'arch': type(self.model).__name__,
            'epoch': epoch,
            'state_dict': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'monitor_best': self.mnt_best,
            'config': self.config
        }
        filename = os.path.join(self.checkpoint_dir, f'checkpoint-epoch{epoch}.pth')
        self.logger.info(f'\nSaving a checkpoint: {filename} ...') 
        torch.save(state, filename)

        if save_best:
            filename = os.path.join(self.checkpoint_dir, f'best_model.pth')
            torch.save(state, filename)
            self.logger.info("Saving current best: best_model.pth")

    def _resume_checkpoint(self, resume_path):
        self.logger.info(f'Loading checkpoint : {resume_path}')
        checkpoint = torch.load(resume_path)
        # Load last run info, the model params, the optimizer and the loggers
        
        self.start_epoch = checkpoint['epoch'] + 1
        self.mnt_best = checkpoint['monitor_best']
        self.not_improved_count = 0

        if checkpoint['config']['arch'] != self.config['arch']:
            self.logger.warning({'Warning! Current model is not the same as the one in the checkpoint'})
        self.model.load_state_dict(checkpoint['state_dict'])

        if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:
            self.logger.warning({'Warning! Current optimizer is not the same as the one in the checkpoint'})
        self.optimizer.load_state_dict(checkpoint['optimizer'])

        self.logger.info(f'Checkpoint <{resume_path}> (epoch {self.start_epoch}) was loaded')

    def _train_epoch(self, epoch):
        raise NotImplementedError

    def _valid_epoch(self, epoch):
        raise NotImplementedError

    def _eval_metrics(self, output, target):
        raise NotImplementedError

```



# dataloaders

## own dataset

在自定义的数据集中，需要写的有两个类，一个是继承BaseDataSet的数据类，用来定位数据位置以及定义读取数据的方法，另一个是继承BaseDataLoader的加载类，用来定义与DataLoader相关的参数，比如batchsize，和一些数据增强的设置

```python
class ClothesDataset(BaseDataSet):

    def __init__(self, **kwargs):
        self.num_classes = 59
        self.palette = _get_palette(self.num_classes)
        super(ClothesDataset, self).__init__(**kwargs)

    def _set_files(self):
        self.root = os.path.join(self.root, 'clothing-co-parsing-master')
        self.image_dir = os.path.join(self.root, 'photos')
        self.label_dir = os.path.join(self.root, 'annotations/pixel-level')
        self.files = [line.split('.')[0] for line in os.listdir(self.image_dir)]

    def _load_data(self, index):
        image_id = self.files[index]
        image_path = os.join(self.image_dir, image_id+'.jpg')
        label_path = os.join(self.label_dir, image_id+'.mat')
        image = np.asarray(Image.open(image_path), dtype=np.float32)
        label = np.asarray(Image.open(label_path, dtype=np.int))
        return image, label, image_id


class Clothes(BaseDataLoader):
    def __init__(self, data_dir, batch_size, crop_size=None, base_size=None, scale=True, num_workers=1, val=False,
                    shuffle=False, flip=False, rotate=False, blur= False, augment=False, val_split= None, return_id=False):
        self.MEAN = [0.55, 0.53374356, 0.526291]
        self.STD = [0.26, 0.257, 0.259]

        kwargs = {
            'root': data_dir,
            'mean': self.MEAN,
            "std": self.STD,
            'augment': augment,
            'crop_size': crop_size,
            'base_size': base_size,
            'scale_size': scale,
            'flip': flip,
            'blur': blur,
            'rotate': rotate,
            'return_id': return_id,
            'val': val
        }
        self.dataset = ClothesDataset(**kwargs)
        super(Clothes, self).__init__(self.dataset, batch_size, shuffle, num_workers, val_split)

```

 # trainer

在自定义的训练器上只需实现初始化函数、每个epoch的训练和评估函数即可

```python
class Trainer(BaseTrainer):
    def __init__(self, model, loss, resume, config, train_loader, val_loader=None, train_logger=None, prefetch=True):
        super(Trainer, self).__init__(model, loss, resume, config, train_loader, val_loader, train_logger)
        
        self.wrt_mode, self.wrt_step = 'train_', 0
        self.log_step = config['trainer'].get('log_per_iter', int(np.sqrt(self.train_loader.batch_size)))
        if config['trainer']['log_per_iter']: self.log_step = int(self.log_step / self.train_loader.batch_size) + 1

        self.num_classes = self.train_loader.dataset.num_classes

        # TRANSORMS FOR VISUALIZATION
        
        self.restore_transform = transforms.Compose([
            local_transforms.DeNormalize(self.train_loader.MEAN, self.train_loader.STD),
            transforms.ToPILImage()])
        self.viz_transform = transforms.Compose([
            transforms.Resize((400, 400)),
            transforms.ToTensor()])
        
        if self.device ==  torch.device('cpu'): prefetch = False
        if prefetch:
            self.train_loader = DataPrefetcher(train_loader, device=self.device)
            self.val_loader = DataPrefetcher(val_loader, device=self.device)

        torch.backends.cudnn.benchmark = True

    def _train_epoch(self, epoch):
        self.logger.info('\n')
            
        self.model.train()
        if self.config['arch']['args']['freeze_bn']:
            if isinstance(self.model, torch.nn.DataParallel): self.model.module.freeze_bn()
            else: self.model.freeze_bn()
        self.wrt_mode = 'train'

        tic = time.time()
        self._reset_metrics()
        tbar = tqdm(self.train_loader, ncols=130)
        for batch_idx, (data, target) in enumerate(tbar):
            self.data_time.update(time.time() - tic)
            #data, target = data.to(self.device), target.to(self.device)
            self.lr_scheduler.step(epoch=epoch-1)

            # LOSS & OPTIMIZE
            
            self.optimizer.zero_grad()
            output = self.model(data)
            if self.config['arch']['type'][:3] == 'PSP':
                assert output[0].size()[2:] == target.size()[1:]
                assert output[0].size()[1] == self.num_classes 
                loss = self.loss(output[0], target)
                loss += self.loss(output[1], target) * 0.4
                output = output[0]
            else:
                assert output.size()[2:] == target.size()[1:]
                assert output.size()[1] == self.num_classes 
                loss = self.loss(output, target)

            if isinstance(self.loss, torch.nn.DataParallel):
                loss = loss.mean()
            loss.backward()
            self.optimizer.step()
            self.total_loss.update(loss.item())

            # measure elapsed time
            
            self.batch_time.update(time.time() - tic)
            tic = time.time()

            # LOGGING & TENSORBOARD
            
            if batch_idx % self.log_step == 0:
                self.wrt_step = (epoch - 1) * len(self.train_loader) + batch_idx
                self.writer.add_scalar(f'{self.wrt_mode}/loss', loss.item(), self.wrt_step)

            # FOR EVAL
            
            seg_metrics = eval_metrics(output, target, self.num_classes)
            self._update_seg_metrics(*seg_metrics)
            pixAcc, mIoU, _ = self._get_seg_metrics().values()
            
            # PRINT INFO
            
            tbar.set_description('TRAIN ({}) | Loss: {:.3f} | Acc {:.2f} mIoU {:.2f} | B {:.2f} D {:.2f} |'.format(
                                                epoch, self.total_loss.average, 
                                                pixAcc, mIoU,
                                                self.batch_time.average, self.data_time.average))

        # METRICS TO TENSORBOARD
        
        seg_metrics = self._get_seg_metrics()
        for k, v in list(seg_metrics.items())[:-1]: 
            self.writer.add_scalar(f'{self.wrt_mode}/{k}', v, self.wrt_step)
        for i, opt_group in enumerate(self.optimizer.param_groups):
            self.writer.add_scalar(f'{self.wrt_mode}/Learning_rate_{i}', opt_group['lr'], self.wrt_step)
            #self.writer.add_scalar(f'{self.wrt_mode}/Momentum_{k}', opt_group['momentum'], self.wrt_step)

        # RETURN LOSS & METRICS
        
        log = {'loss': self.total_loss.average,
                **seg_metrics}

        #if self.lr_scheduler is not None: self.lr_scheduler.step()
        
        return log

    def _valid_epoch(self, epoch):
        if self.val_loader is None:
            self.logger.warning('Not data loader was passed for the validation step, No validation is performed !')
            return {}
        self.logger.info('\n###### EVALUATION ######')

        self.model.eval()
        self.wrt_mode = 'val'

        self._reset_metrics()
        tbar = tqdm(self.val_loader, ncols=130)
        with torch.no_grad():
            val_visual = []
            for batch_idx, (data, target) in enumerate(tbar):
                #data, target = data.to(self.device), target.to(self.device)
                
                # LOSS
                
                output = self.model(data)
                loss = self.loss(output, target)
                if isinstance(self.loss, torch.nn.DataParallel):
                    loss = loss.mean()
                self.total_loss.update(loss.item())

                seg_metrics = eval_metrics(output, target, self.num_classes)
                self._update_seg_metrics(*seg_metrics)

                # LIST OF IMAGE TO VIZ (15 images)
                
                if len(val_visual) < 15:
                    target_np = target.data.cpu().numpy()
                    output_np = output.data.max(1)[1].cpu().numpy()
                    val_visual.append([data[0].data.cpu(), target_np[0], output_np[0]])

                # PRINT INFO
                
                pixAcc, mIoU, _ = self._get_seg_metrics().values()
                tbar.set_description('EVAL ({}) | Loss: {:.3f}, PixelAcc: {:.2f}, Mean IoU: {:.2f} |'.format( epoch,
                                                self.total_loss.average,
                                                pixAcc, mIoU))

            # WRTING & VISUALIZING THE MASKS
            
            val_img = []
            palette = self.train_loader.dataset.palette
            for d, t, o in val_visual:
                d = self.restore_transform(d)
                t, o = colorize_mask(t, palette), colorize_mask(o, palette)
                d, t, o = d.convert('RGB'), t.convert('RGB'), o.convert('RGB')
                [d, t, o] = [self.viz_transform(x) for x in [d, t, o]]
                val_img.extend([d, t, o])
            val_img = torch.stack(val_img, 0)
            val_img = make_grid(val_img.cpu(), nrow=3, padding=5)
            self.writer.add_image(f'{self.wrt_mode}/inputs_targets_predictions', val_img, self.wrt_step)

            # METRICS TO TENSORBOARD
            
            self.wrt_step = (epoch) * len(self.val_loader)
            self.writer.add_scalar(f'{self.wrt_mode}/loss', self.total_loss.average, self.wrt_step)
            seg_metrics = self._get_seg_metrics()
            for k, v in list(seg_metrics.items())[:-1]: 
                self.writer.add_scalar(f'{self.wrt_mode}/{k}', v, self.wrt_step)

            log = {
                'val_loss': self.total_loss.average,
                **seg_metrics
            }

        return log

    def _reset_metrics(self):
        self.batch_time = AverageMeter()
        self.data_time = AverageMeter()
        self.total_loss = AverageMeter()
        self.total_inter, self.total_union = 0, 0
        self.total_correct, self.total_label = 0, 0

    def _update_seg_metrics(self, correct, labeled, inter, union):
        self.total_correct += correct
        self.total_label += labeled
        self.total_inter += inter
        self.total_union += union

    def _get_seg_metrics(self):
        pixAcc = 1.0 * self.total_correct / (np.spacing(1) + self.total_label)
        IoU = 1.0 * self.total_inter / (np.spacing(1) + self.total_union)
        mIoU = IoU.mean()
        return {
            "Pixel_Accuracy": np.round(pixAcc, 3),
            "Mean_IoU": np.round(mIoU, 3),
            "Class_IoU": dict(zip(range(self.num_classes), np.round(IoU, 3)))
        }
```

# inferance

推理阶段一共分为：读取测试数据集，对数据进行预处理，构建并加载模型，预测模型，保存结果

1. 读取数据集

   ```python
   #确定数据集种类，选择对应的dataloader
   
   loader = dataloaders.Clothes(datadir=datadir, batch_size=batch_size)
   to_tensor = transforms.ToTensor()
   normalize = transforms.Normalize(mean=loader.MEAN, std=loader.STD)
   num_classes = loader.dataset.num_classes
   #对应数据集的调色板
   
   palette = loader.dataset.palette
   model = models.UNet(num_classes=num_classes)
   gpus_available = list(range(torch.cuda.device_count()))
   device = torch.device('cuda:0' if len(gpus_available) > 0 else 'cpu')
   ```

2. 构建并加载模型

   ```python
   # 当训练保存模型不是采用分布式的情况
   checkpoint = torch.load(args.model)
   if isinstance(checkpoint, dict) and 'state_dict' in checkpoint.keys():
       checkpoint = checkpoint['state_dict']
   
       # 如果采用多gpu分布式训练时，并且保存模型采用的model.state_dict()而不是model.module.state.dict()
       if 'module' in list(checkpoint.keys())[0] and not isinstance(model, torch.nn.DataParallel):
   
           # 将模型用DataParallel进行包装，并行训练
           if "cuda" in device.type:
               model = torch.nn.DataParallel(model)
               else:
   
                   # 采用cpu进行训练，此时需要将checkpoint中的key值去掉最前面的'module.'即可
                   new_state_dict = OrderedDict()
                   for k, v in checkpoint.items():
                       # 去掉
                       name = k[7:]
                       new_state_dict[name] = v
                       checkpoint = new_state_dict
                       model.load_state_dict(checkpoint)
                       model.to(device)
                       model.eval()
   ```

3. 模型预测

   ```python
   # 这里用'output'而不用args.output是因为避免用错误输入导致创建了过多的文件夹
   
   if not os.path.exists('output'):
       os.makedirs('outputs')
   image_files = sorted(glob(os.path.join(args.images, f'*.{args.extension}')))
   with torch.no_grad():
       tbar = tqdm(image_files, ncols=100)
       for img_file in tbar:
           image = Image.open(img_file).convert('RGB')
           input = normalize(to_tensor(image)).unsqueeze(0)
   		#采用多尺度预测， 再将多尺度的预测结果还原成原图大小取平均，得到最终的预测结果 
           
           if args.mode == 'multiscale':
               prediction = multi_scale_predict(model, input, scales, num_classes, device)
               # 
               
               elif args.mode == 'sliding':
                   prediction = sliding_predict(model, input, num_classes)
                   else:
                       prediction = model(input.to(device))
                       prediction = prediction.squeeze(0).cpu().numpy()
                       prediction = F.softmax(torch.from_numpy(prediction), dim=0).argmax(0).cpu().numpy()
                       save_images(image, prediction, args.output, img_file, palette)
   ```

   多尺度预测：

   ```python
   def multi_scale_predict(model, image, scales, num_classes, device, flip=False):
       input_size = (image.size(2), image.size(3))
       upsample = nn.Upsample(size=input_size, mode='bilinear', align_corners=True)
       total_predictions = np.zeros((num_classes, image.size(2), image.size(3)))
   
       image = image.data.data.cpu().numpy()
       for scale in scales:
           # 这里其实跟cv2.resize()功能一样，之所以不用是因为这里所有图片的格式都是RGB格式，而opencv都是BGR格式
           
           scaled_img = ndimage.zoom(image, (1.0, 1.0, float(scale), float(scale)), order=1, prefilter=False)
           scaled_img = torch.from_numpy(scaled_img).to(device)
           scaled_prediction = upsample(model(scaled_img).cpu())
   		# 这里在多尺度的基础下再加上镜像，最后将预测结果取平均
           
           if flip:
               fliped_img = scaled_img.flip(-1).to(device)
               fliped_predictions = upsample(model(fliped_img).cpu())
               scaled_prediction = 0.5 * (fliped_predictions.flip(-1) + scaled_prediction)
           total_predictions += scaled_prediction.data.cpu().numpy().squeeze(0)
   
       total_predictions /= len(scales)
       return total_predictions
   ```

   

# brainstorming

* *args,**kwargs的作用：

  \*args是位置参数组成的元组，\*\*kwargs是以字典形式保存的键值对参数，两者同时调用时，\*args要放在前面，主要用于子类继承父类是接受父类方法中的各种参数
  
* \_\_repr\_\_()方法是用来在交互模式下更好的显示对象的属性，比如直接在交互模式下,直接输入对象，调用的就是r\_\_repr\_\_()方法

  ```
  dataset = Dataset()
  dataset
  <object ...>
  ```

  与\_\_repr\_\_()类似的还有\_\_str\_\_()方法，如果采用print打印对象或者str()格式化对象的话调用的就是\_\_str\_\_()方法。若对于一个对象容器，比如装了许多对象的list，打印属性的话调用的是\_\_repr\_\_()方法

* self.\_\_class\_\_:返回实例变量对应的类

* logging模块

  日志级别（由低到高）：

  * DEBUG：在对程序进行状态监控，诊断问题的时候使用
  * INFO：确认代码当前的运行情况
  * WARNING：基于一个特殊的运行时事件提出一个警告信息
  * ERROR：基于一个特殊的运行时事件报告一个错误
  * CRITICAL：报告一个严重的错误，表明程序已经不能再继续执行

  注意以上都只是输出信息，并**不会**自动中断程序

  ### 基本使用方法：

  1. 直接输出信息到控制台

     ```python
     logging.warning('hello')
     logging.info('world')
     ```

     输出可能只有hello，因为logging默认的输出级别是warning级别

  2. 记录日志到文件

     ```
     logging.basicConfig(filename='event.log', level=logging.DEBUG)
     logging.debug('1')
     logging.info('2')
     logging.warning('3')
     ```

  3. 更改显示消息的格式

     ```python
     logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
     logging.info('Hello')
     >>>INFO:Hello
     logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG)
     logging.info('Hello')
     >>>2010-12-12 11:41:42,612 INFO:Hello
     ```

  ### 进阶使用方法：

  logging库提供四大类组件：

  1. 记录器：是日志消息和处理程序、过滤器、格式化程序之间的桥梁

     通过调用logger类的实例来生成不同的记录器，每一个记录器都有一个层次结构，以句点为分隔符，比如名为‘a'的记录器就是名为'a.b'记录器的父级

     常见的声明方法是使用模块级记录器：

     ```
     logger = logging.getLogger(__name__)
     ```

     记录器的配置方法：

     * logger.setLevel(): 指定记录器的最低严重性
     * Logger.addHandler(),Logger.removeHandler():用来从记录器对象中增加或者删除相应的处理程序对象
     * Logger.addFilter(),Logger.removeFilter():添加或者移除记录器对象中的过滤器

  2. 处理程序：基于日志的严重性将日志消息分派给处理程序的制定目标

     ```python
     ch = logging.StreamHandler()
     ch.setLevel(logging.DEBUG)
     logger.addHandler(ch)
     ```

     不应该直接实例化或者使用Handler的实例，Handler是一个基类，它定义了所有处理程序应该既有的接口

  3. 过滤器：提供更加精细的附加功能，用于确定要输出的日志内容

  4. 格式化程序:指定最终输出的日志记录的样式

     格式化程序对象配置日志消息的最终顺序、结构和内容，与Handler不同，可以对格式化程序类进行实例化

     ```python
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     ch.addFormatter(formatter)
     ```

* pytorch多gpu并行

  pytorh提供了封装好的接口DataParallel,只需要将model实例放入该容器中即可：

  ```python
  model = Model(input_size, output_size)
  model = nn.DataParallel(model)
  model.to('cuda')
  ...
  for data in data_loader:
      input = data.to('cuda')
      output = model(input)
  ```

  实现的原理就是在forward阶段将对应的层加载到其他gpu上然后将输入的一个batch_size均分到多个gpu上进行计算(所以batch_size数要大于并行的gpu数量)，再将梯度求和传回第一个gpu上，再销毁多余的层

  ![](D:\Documents\GitHub\L162534.github.io\img\gpu-1.jpg)

  这种方法带来的局限就是在batchnorm层的实现上让输入数据出现偏差，原本是统计整个batch_size的再进行归一化，现在只能分开统计，从而出现误差

  网上有人解决这个问题，解决的流程如下：

  ![](D:\Documents\GitHub\L162534.github.io\img\gpu-2.jpg)

* ```python
  torch.backends.cudnn.benchmark = True
  ```

  这句话可以加快模型的运行速度，但如果模型的数据结构在不断变化，这会导致模型的运行速度更慢。这句代码能加快运行速度是因为：程序最开始会对网络中的各个卷积层进行多种方式的卷积运算，通过对比选取最优的卷积运算方式，从而实现加速，如果卷积层的参数不断在变化，会导致程序不停地做优化对比，反而会耗费更多时间。

* PIL库采用调色板模式显示图片

  ```python
  img = Image.open(path)
  img = img.convert("P")
  # 调色板是颜色种类的3倍，图片调用调色板显示就是通过原图的像素值作为索引去找相应的RGB彩色像素值
  
  palette = [255,0,0, 0,255,0, 0,0,255,...]
  img.putpalette(palette)
  ```

  

