---
layout:  post
title:  日常
subtitle:  SegNet
date:  2021-04-25
author:  LY
catalog:  true
tags:
    -语义分割
---

# SegNet

创新点：解码器网络， 上采样通过记住下采样时对应的索引直接恢复部分的像素点，减轻了完全由kernel学习出上采样结果的负担，具体的实现过程：

1. 某一个4x4的特征图经过最大值池化后，会返回池化后的特征图和相应的索引数组，比如a在2x2的池化核中的坐标是(0,0),b在2x2的池化核中的坐标是(0,1),c(0,1),d(1,0),这个坐标会存在一个数组中

   ```python
   # 保存特征图像素和对应的索引
   
   x1p, id1 = torch.nn.functional.max_pool2d(x12,kernel_size=2, stride=2,return_indices=True)
   ```

2. 根据相应的索引数组将待上采样的像素安插在索引数组指定的位置

   ```python
   #x21d是上一层的特征图
   
   x1d = torch.nn.functional.max_unpool2d(x21d, id1, kernel_size=2, stride=2)
   ```

   

![](D:\Documents\GitHub\L162534.github.io\img\SegNet.jpg)

这样的上采样的方式是带来的好处包括：

1. 让边界的细节更多
2. 减少了实现端到端训练的参数量(语义分割的参数量往往非常大，使得端到端训练非常困难，以FCN为代表的端到端训练方式是分阶段训练的，否则难以取得理想效果)
3. 解码器的上采样形式很容易应用到其他网络结构当中去

# DeepLab

## DeepLab V1

创新点：结合了深度卷积网络和全连接的CRF模型，目的是解决定位与抽象之间的关系：重复的下采样会丢失细节，DeepLab采用带孔卷积来扩展感受野；分类器获取目标对象的中心位置网络需要空间变换的不变性，这就限制了定位精度，相当于用牺牲定位精度来换取中心定位的准确度，DeepLab采用全连接的条件随机场(DenseCRF)来提高模型捕获细节的能力，将class score和低层次的信息结合起来

## DeepLab V2

改进点：

1. 使用空洞空间卷积池化金字塔(ASPP)来获得更好的分割效果
2. 主干网络从VGG16变为ResNet

## DeepLab V3

改进点：

1. 改进了ASPP模块，由不同采样率的空洞卷积和BN层组成

2. 发现一个问题：使用大采样率的3x3空洞卷积，ui为图像边界的原因无法捕捉远距离的信息会退化成1x1的卷积(卷积核四周的权重几乎为0)，论文提出将图像级特征融合到ASPP模块中

3. 对比了级联ASPP和并联ASPP的结构，并且在同一层中也采用不同空洞率的空洞卷积

4. 阐述了训练的细节

   * 学习率采用poly策略：学习率乘以下述因子
     $$
     （1-\frac{iter}{max\_iter})^{power}
     $$
     power取0.9

   * 为了让大采样率的空洞卷积有效，往往需要用较大尺寸的图片

   * 在ResNet后添加的模块均搭建了BN层，将output_stride(输入尺寸与输出尺寸的比率)设为16，BN层的decay设为0.9997，训练的学习率为0.007，迭代了30K iteration，batchsize为16，然后冻结BN层，将output_stride设为8，用0.001的学习率再训练30K iteration

   * 之前的工作是将groundtruth缩小成模型输出的大小进行比较，但是论文发现用模型的输出进行上采样到和groundtruth相同尺寸进行比较效果更好，因为对groundtruth进行下采样损失了精细度

   * 对原始训练数据进行水平翻转和0.5到2的随机尺度变换

# PSPNet

创新点：

* 之前的分割模型主要是基于两个方面展开工作：一是将低层次的特征和高层次的特征进行融合，另一个就是基于结构预测，比如使用CRF做后端细化，为了充分利用全局特征来对场景进行理解，PSP模块能够聚合不同区域的上下文从而达到获取全局上下文的目的。
* 采用中继监督loss，加快训练的收敛

模型结构：

![](D:\Documents\GitHub\L162534.github.io\img\PSPNet.png)

原图经过ResNet和空洞卷积提取出Feature Map，Feature Map的尺寸是原图的1/8，再经过中间的PSP模块得到融合的带有整体信息的feature，再与Feature Map进行拼接，最后通过卷积层进行输出

## Pyramid Pooling Module

* 采用多个种类的pool后的特征尺寸，包含1x1，2x2，3x3，6x6
* 为了保持全局特征的权重，我们用1x1卷积将每个金字塔的权重乘以1/N，N代表金字塔的层次数
* 将每个金字塔池化后的特征图进行上采样，使之尺寸与Feature Map相同，以便后续拼接



# brain storming

* 端到端训练：

  就是直接将输入数据丢入模型，让模型直接输出期望的输出，这样的好处是可以带来最好的优化效果，比如如果把某一个任务分成两个部分，前一部分最优的状态可能并不是后一部分状态最优的部分，端到端训练就是为了找到在所有步骤中找到全局最优的状态；坏处就是这会让模型越来越像一个黑箱，越来越难以解释，比如最开始是将图片经过人工设计特征再通过分类器去分类，如今直接输入图片让机器自己去找到最适合的特征去分类。