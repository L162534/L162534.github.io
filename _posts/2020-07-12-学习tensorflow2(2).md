---
layout:  post
title:  学习tensorflow2(2)
subtitle:  抓紧学！来不及了
date:  2020-07-12
author:  L&X
catalog:  true
tags:
    -深度学习框架
---

## 深度学习的关键点：

* 数据集（data）

* 转换数据的模型（model）
* 衡量模型好坏的损失函数（loss）
* 调整模型参数以减小损失函数的算法（algorithms）

## tensorflow的broadcast机制

当两个不同维度的向量进行操作时，当满足以下两个条件即通过broadcast机制自动填充，使得两个向量的维度一致：

1. 从低维度向高维度遍历均一致

   ```python
   A = tf.zeros((1,4)) 
   B = tf.zeros((1,2,3,4))
   C = A + B  #可broadcast
   A = tf.zeos((2,4))
   C = A + B  #不可broadcast
   ```

2. 若不满足条件1，维度不同的两个值若其中有一个包含1，则也满足条件

   ```python
   A = tf.zeros((1,4)) 
   B = tf.zeros((1,2,3,4))
   C = A + B  #可broadcast
   ```


## 张量的索引

张量即可从正反向也可从反方向索引元素

```python
x = tf.range(12)
y = tf.reshape(x,(3,4))
y[0] = [0,1,2,3,]
y[-1] = [8,9,10,11]
```

这里要特别注意如果要引用类似y[0:2],是不包括最后的元素的，只包含y[0]和y[1]

```
y[0:2] = [[0,1,2,3],
		  [4,5,6,7]]
```

## tensor、array、scalar、variable之间的转换

* tensor转array

  ```
  a = tf.range(12)
  b = a.numpy() 
  ```

* array转scalar（仅适用size为1的array,因为scalar就是单个元素）

  ```
  c = b.item()
  c = int(b)
  ```

* tensor转variable

  ```
  d = tf.Variable(a)
  ```

  tensor是没有赋值的方法，如果要改变tensor的值要先转换成variable，调用assign方法，再将variable转换成tensor。

* variable转tensor

  ```
  a = tf.convert_to_tensor(d)
  ```

## pandas处理数据集

* 读取csv文件

    ```python
    import pandas as pd

    data = pd.read_csv(data_file)
    ```

* 获取指定列的数据

  ```python
  input = data.iloc[:, 0:2]
  ```

* 获取指定行数据

    ```python
    input = data.iloc[0:10, :]
    ```

    数据集中经常会出现数据缺失的现象，读取数据后，默认会给缺失的值赋“NaN”，通常有两种做法：删除法和插补法

    插补法：用其他值代替缺失的值

    ```python
    input = input.fillna(1) # 用1去填补
    ```

* 将数据集数据转换成tensor

  ```python
  x = tf.constant(input.values)
  ```

## 基本运算

* 求和

  ```python
  sum = tf.reduce_sum(A, axis = m, keepdims = True)
  ```

  A是tensor， axis是选择沿着哪些轴求和，keepdims用来设置是否保留原矩阵的维度，求和后维度肯定会变小，但是如果保持维度不变，就可以运用broadcast机制进行类似A/sum的运算

* 矩阵乘积

  C= AxB

  1. tensordot，矩阵A的列不用等于B的行

      ```python
      A = tf.ones((2,3,4))
      B = tf.ones((6,2))
      tf.tensordot(A,B,axes=2) #A的后两个维度合并，B的前两个维度合并，（2，12）*（12，）->(2,)
      tf.tensordot(A,B,axes=([1,2],[0,1])) #将A的第1，2维度合并，B的第0，1维度合并
      tf.tensordot(A,B,axes=([1,2],[1,0]))  #注意这里将B按照第1，0维度合并，和上面得到的结果是不同的，一个按照行展开，一个按照列展开
      ```
      
  2. matmul,A的列必须等于B的行
  
      ```
      tf.matmul(A,B)
      ```

## 范数

L1范数：

```
tf.norm(x) # 求平方和再开方
```

L2范数：

```
tf.reduce_sum(tf.abs(x)) # 绝对值求和
```

## 上下文管理器

用来连接需要求解梯度的函数和变量，tf.1x采用的是静态图模式，分为前向图和反向图，其中反向图用来求解梯度，tf.2x则采用tf.GradientTape()来自动管理梯度的求解

```python
x = tf.Variable(3.0) # tf.GradientTape默认监控variable变量中trainable属性为True的变量
with tf.GradientTape as Tape:
    y = x*x
dy_dx = Tape.gradient(y,x)
```

如果要求多阶导数的话：

```python
x = tf.Variable(3.0)
with tf.GradientTape() as Tape: 
    y = x*x
    with tf.GradientTape() as Tape1:
        y = x*x
        dy_dx = Tape.gradient(y,x)
d2y_dx2 = Tape1.gradient(dy_dx, x)
d2y_dx2 # <tf.Tensor: id=715, shape=(), dtype=float32, numpy=2.0>
```

分离计算：

如果y是关于x的函数，z是关于x，y的函数，如果我们想知道将y视作常数，z对x的导数:

```python
with tf.GradientTape(persistent=True) as t:
    y = x * x
    u = tf.stop_gradient(y)
    z = u * x

x_grad = t.gradient(z, x)
x_grad == u
```

